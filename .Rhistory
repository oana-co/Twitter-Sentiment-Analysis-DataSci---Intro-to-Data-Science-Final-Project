dateDownloaded <- date()
install.packages("xlsx")
library(xlsx)
cameraData <- read.xlsx("./data/cameras.xlsx", sheetIndex=1, header=TRUE)
head(cameraData)
options(java.parameters = "-Xmx1000m")
colIndex <- 2:3
rowIndex <- 1:4
cameraDataSubset <- read.xlsx("./data/cameras.xlsx", sheetIndex=1,
colIndex=colIndex, rowIndex=rowIndex)
cameraDataSubset
library(XML)
install.packages("XML")
library(XML)
fileUrl <-"http://www.w3school.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
fileUrl <-"http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNote)
names(rootNode)
rootNode[[1]]
rootNode[[1]] [[1]]
xmlApply(rootNode, xmlValue)
xapthSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//price", xmlValue)
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl, useInternal=TRUE)
socres <- xpathSApply(doc, "//li[@calss='score']",xmlValue)
scores <- xpathSApply(doc, "//li[@calss='score']",xmlValue)
scores <- xpathSApply(doc, "//li[@class='score']",xmlValue)
teams <- xpathSApply(doc, "//li[@class='team']",xmlValue)
scores
scores <- xpathSApply(doc, "//li[@class='score']", xmlValue)
teams <- xpathSApply(doc, "//li[@class='team']", xmlValue)
scores
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
scores <- xpathSApply(doc,"//li[@class='score']", xmlValue)
teams <- xpathSApply(doc,"//li[@class='team']", xmlValue)
scores <- xpathSApply(doc,"//li[@class='score']", xmlValue)
teams <- xpathSApply(doc,"//li[@class='team']", xmlValue)
scores
team
teams
install.packages(jsonLite)
install.packages(jsonlite)
install.packages("jsonlite")
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
install.packages('httr')
library(httr)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
View(jsonData)
names(jsonData)
myjson <- taJSON(iris, pretty=TRUE)
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
iris2 <- fromJSON(myjson)
head(iris2)
library(data.table)
DF = data.frame(x=rnorm(9), y=rep(c("a", "b", "c"),each=3), z=rnorm(9))
DF = data.frame(x=rnorm(9), y=rep(c("a", "b", "c"),each=3), z=rnorm(9))
head(DF)
DT = data.table(x=rnorm(9), y=rep(c("a", "b", "c"),each=3), z=rnorm(9))
head(DT)
tables()
DT[2,]
DT[DT$y=="a"]
DT[,w:=z*2]
DT
set.seed(123);
DT <- data.table(x=smaple(letters[1:3], 1E5, TRUE))
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT [, .N, by=x]
DT <- data.table(x=rep(c("a","b","c", each=100, ynorm(300))
DT <- data.table(x=rep(c("a","b","c"), each=100, ynorm(300))
DT <- data.table(x=rep(c("a","b","c"), each=100, ynorm(300))
setkey(DT, x)
setkey(DT, x)
DT['a']
install.packages("RMySQL")
library(RMySQL)
host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb);
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes=T)
xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
xpathSApply(parsedHtml, "//title", xmlValue)
library(httr); html2 = GET(url)
content2 = content(html2,as="text")
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
pg2 = GET("http://httpbin.org/basic-auth/user/passwd",
authenticate("user","passwd"))
pg2
names(pg2)
google = handle("http://google.com")
pg1 = GET(handle=google,path="/")
pg2 = GET(handle=google,path="search")
myapp = oauth_app("twitter",
key="q0wvm80ceeCGml0UmK7Dl7hvP",secret="LE3RvKLuzUncx4T6AjpHNMlhNHABNEtVg9lriiJWLHyD18YS6B")
sig = sign_oauth1.0(myapp,
token = "54455879-yPWU2L26jrBGSuh5duFe10aE3KkTlCI17lftzBOUw",
token_secret = "TCG72FRR5x3NUEtiFdv5Nq5fbUasenbiUJSx61Txdk5ub")
3
5
yes
myapp = oauth_app("twitter",
key="q0wvm80ceeCGml0UmK7Dl7hvP",secret="LE3RvKLuzUncx4T6AjpHNMlhNHABNEtVg9lriiJWLHyD18YS6B")
sig = sign_oauth1.0(myapp,
token = "54455879-yPWU2L26jrBGSuh5duFe10aE3KkTlCI17lftzBOUw",
token_secret = "TCG72FRR5x3NUEtiFdv5Nq5fbUasenbiUJSx61Txdk5ub")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
myapp = oauth_app("twitter",
key="q0wvm80ceeCGml0UmK7Dl7hvP",secret="LE3RvKLuzUncx4T6AjpHNMlhNHABNEtVg9lriiJWLHyD18YS6B")
sig = sign_oauth1.0(myapp,
token = "54455879-yPWU2L26jrBGSuh5duFe10aE3KkTlCI17lftzBOUw",
token_secret = "TCG72FRR5x3NUEtiFdv5Nq5fbUasenbiUJSx61Txdk5ub")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
json2[1,1:4]
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
set.seed(13435)
X <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
X <- X[sample(1:5),]; X$var2[c(1,3)] = NA
X <- data.frame("var1"=sample(1:5),"var2"=sample(6:10),"var3"=sample(11:15))
X <- X[sample(1:5),]; X$var2[c(1,3)] = NA
X
library(plyr)
arrange(X,var1)
X$var4 <- rnorm(5)
Y <- cbind(X,rnorm(5))
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
restData <- read.csv("./data/restaurants.csv")
View(restData)
summary(restData)
str(restData)
quantile(restData$councilDistrict,na.rm=TRUE)
quantile(restData$councilDistrict,probs=c(0.5,0.75,0.9))
table(restData$zipCode,useNA="ifany")
table(restData$councilDistrict,restData$zipCode)
colSums(is.na(restData))
all(colSums(is.na(restData))==0)
table(restData$zipCode %in% c("21212"))
table(restData$zipCode %in% c("21212","21213"))
restData[restData$zipCode %in% c("21212","21213"),]
data(UCBAdmissions)
DF = as.data.frame(UCBAdmissions)
summary(DF)
xt <- xtabs(Freq ~ Gender + Admit,data=DF)
xt
warpbreaks$replicate <- rep(1:9, len = 54)
xt = xtabs(breaks ~.,data=warpbreaks)
xt
ftable(xt)
if(!file.exists("./data")){dir.create("./data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/restaurants.csv",method="curl")
restData <- read.csv("./data/restaurants.csv")
s1 <- seq(1,10,by=2) ; s1
s2 <- seq(1,10,length=3); s2
x <- c(1,3,8,25,100); seq(along = x)
restData$nearMe = restData$neighborhood %in% c("Roland Park", "Homeland")
table(restData$nearMe)
restData$zipWrong = ifelse(restData$zipCode < 0, TRUE, FALSE)
table(restData$zipWrong,restData$zipCode < 0)
restData$zipGroups = cut(restData$zipCode,breaks=quantile(restData$zipCode))
table(restData$zipGroups)
table(restData$zipGroups,restData$zipCode)
library(Hmisc)
restData$zipGroups = cut2(restData$zipCode,g=4)
table(restData$zipGroups)
restData$zcf <- factor(restData$zipCode)
restData$zcf[1:10]
class(restData$zcf)
yesno <- sample(c("yes","no"),size=10,replace=TRUE)
yesnofac = factor(yesno,levels=c("yes","no"))
relevel(yesnofac,ref="yes")
as.numeric(yesnofac)
library(Hmisc); library(plyr)
restData2 = mutate(restData,zipGroups=cut2(zipCode,g=4))
table(restData2$zipGroups)
forecast(RegJanuary, 240)
## Read the data
bike_train<-read.csv("train.csv",stringsAsFactors=FALSE,header=TRUE)
str(bike_train)
## Read columns names
names(bike_train)
## Read first 10 rows of data
head(bike_train, n=10)
str(bike_train)
## Install packages
install.packages("tseries", "forecast", dependencies=TRUE)
install.packages("forecast")
library(forecast)
#plot only Registered Users
RegUsers <- bike_train$registered
plot(RegUsers)
## Store data in a column (say RegUsersDaily) as time series daily data:
RegUsersDaily <- ts(bike_train$registered, frequency=7)
plot(RegUsersDaily)
RegUsersMonthly <- ts(bike_train$registered, frequency=30)
plot(RegUsersMonthly)
RegUsersYearly <- ts(bike_train$registered, frequency=365.25)
plot(RegUsersYearly)
## Seasonality:
RegUsersSeas <- msts(bike_train$registered, seasonal.periods=c(7,365.25))
## Perform auto arima on Daily Registered time series data using auto.arima() function of “forecast” package
auto.arima(RegUsersDaily)
## We get: => weekly seasonality
## ARIMA(5,1,5)(2,0,2)[7] with drift
## Coefficients:
##   ar1     ar2     ar3      ar4      ar5     ma1      ma2      ma3     ma4     ma5
## -0.0557  0.6858  0.6282  -0.1734  -0.4665  0.2412  -1.1072  -1.1139  0.2218  0.8154
## s.e.   0.0124  0.0123  0.0113   0.0114   0.0121  0.0091   0.0083   0.0082  0.0096  0.0081
## sar1    sar2     sma1     sma2   drift
## 0.0814  0.6264  -0.3670  -0.4717  0.0162
## s.e.  0.0231  0.0173   0.0228   0.0169  0.0585
## sigma^2 estimated as 5383:  log likelihood=-62198.37
## AIC=124428.7   AICc=124428.8   BIC=124545.5
## ALTERNATIVE
RegUsers.fit <- tbats(RegUsersDaily)
## Forecast it:
RegUsers.fc <- forecast(RegUsers.fit)
## Plot Registered Users Forecast:
plot(RegUsers.fc)
## Seasonal decomposition, use the decompose() function in R
RegUsersDaily.dc <- decompose(RegUsersDaily)
## To print the first 10 estimated values of the seasonal component:
head(RegUsersDaily.dc$seasonal, n=10)
## We get:
## [1] -0.9636416 -1.2614905  0.7981712  1.3946182 -0.4845734  0.7993663 -0.2824502 -0.9636416
## [9] -1.2614905  0.7981712
## Plot the decomposed series:
plot(RegUsersDaily.dc)
========================
## 3 March ##########
## Subset the data for the first 20 days in the month
## JANUARY ##############
actualJanuary <- subset(bike_train,
select = c("datetime", "registered"))
onlyJanuary <- actualJanuary[1:431, ]
RegJanuary <- ts(onlyJanuary$registered, frequency=24)
plot(RegJanuary)
## Perform auto arima on Daily Registered January time series data
## using auto.arima() function of “forecast” package
auto.arima(RegJanuary)
## Alternative:
library(forecast)
forecast(RegJanuary, 240)
plot(forecast(RegJanuary, 240))
## FEBRUARY ##############
onlyFebruary <- actualJanuary[432:877, ]
RegFebruary <- ts(onlyFebruary$registered, frequency=24)
plot(RegFebruary)
## Perform auto arima on Daily Registered February time series data
## using auto.arima() function of “forecast” package
auto.arima(RegFebruary)
## Alternative:
library(forecast)
forecast(RegFebruary, 240)
plot(forecast(RegFebruary, 240))
## MARCH ##############
onlyMarch <- actualJanuary[878:1323, ]
RegMarch <- ts(onlyMarch$registered, frequency=24)
plot(RegMarch)
## Perform auto arima on Daily Registered February time series data
## using auto.arima() function of “forecast” package
auto.arima(RegMarch)
## Alternative:
library(forecast)
forecast(RegMarch, 240)
plot(forecast(RegMarch, 240))
## APRIL ##############
onlyApril <- actualJanuary[1324:1768, ]
RegApril <- ts(onlyApril$registered, frequency=24)
plot(RegApril)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegApril)
## Alternative:
library(forecast)
forecast(RegApril, 240)
plot(forecast(RegApril, 240))
## May ##############
onlyMay <- actualJanuary[1769:2213, ]
RegMay <- ts(onlyMay$registered, frequency=24)
plot(RegMay)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegMay)
## Alternative:
library(forecast)
forecast(RegMay, 240)
plot(forecast(RegMay, 240))
## JUNE ##############
onlyJune <- actualJanuary[2214:2658, ]
RegJune <- ts(onlyJune$registered, frequency=24)
plot(RegJune)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegJune)
## Alternative:
library(forecast)
forecast(RegJune, 240)
plot(forecast(RegJune, 240))
## JULY #############
onlyJuly <- actualJanuary[2659:3103, ]
RegJuly <- ts(onlyJune$registered, frequency=24)
plot(RegJuly)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegJuly)
## Alternative:
library(forecast)
forecast(RegJuly, 240)
plot(forecast(RegJuly, 240))
## AUGUST #############
onlyAugust <- actualJanuary[3104:3548, ]
RegAugust <- ts(onlyAugust$registered, frequency=24)
plot(RegAugust)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegAugust)
## Alternative:
library(forecast)
forecast(RegAugust, 240)
plot(forecast(RegAugust, 240))
## SEPTEMBER ##########
onlySeptember <- actualJanuary[3549:3993, ]
RegSeptember <- ts(onlySeptember$registered, frequency=24)
plot(RegSeptember)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegSeptember)
## Alternative:
library(forecast)
forecast(RegSeptember, 240)
plot(forecast(RegSeptember, 240))
## OCTOBER ##########
onlyOctober <- actualJanuary[3994:4438, ]
RegOctober <- ts(onlyOctober$registered, frequency=24)
plot(RegOctober)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegOctober)
## Alternative:
library(forecast)
forecast(RegOctober, 240)
plot(forecast(RegOctober, 240))
## NOVEMBER ##########
onlyNovember <- actualJanuary[4439:4883, ]
RegNovember <- ts(onlyNovember$registered, frequency=24)
plot(RegNovember)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegNovember)
## Alternative:
library(forecast)
forecast(RegNovember, 240)
plot(forecast(RegNovember, 240))
## DECEMBER ##########
onlyDecember <- actualJanuary[4884:5328, ]
RegDecember <- ts(onlyDecember$registered, frequency=24)
plot(RegDecember)
## Perform auto arima on Daily Registered April time series data
## using auto.arima() function of “forecast” package
auto.arima(RegDecember)
## Alternative:
library(forecast)
forecast(RegDecember, 240)
plot(forecast(RegDecember, 240))
RegUsersDaily <- ts(bike_train$registered, frequency=7)
install.packages("twitteR")
install.packages("wordcloud")
install.packages("RColorBrewer")
install.packages("plyr")
install.packages("ggplot2")
install.packages("sentiment")
install.packages("httr")
install.packages("devtools")
install_github("twitteR", username="geoffjentry")
install.packages("httr")
install.packages("httr")
install.packages("httr")
install.packages("httr")
library(httr)
oauth_endpoints("twitter")
api_key <- "q0wvm80ceeCGml0UmK7Dl7hvP"
api_secret <- "LE3RvKLuzUncx4T6AjpHNMlhNHABNEtVg9lriiJWLHyD18YS6B"
access_token <- "54455879-yPWU2L26jrBGSuh5duFe10aE3KkTlCI17lftzBOUw"
access_token_secret <- "TCG72FRR5x3NUEtiFdv5Nq5fbUasenbiUJSx61Txdk5ub"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
oauth_endpoints("twitter")
api_key <- "q0wvm80ceeCGml0UmK7Dl7hvP"
api_secret <- "LE3RvKLuzUncx4T6AjpHNMlhNHABNEtVg9lriiJWLHyD18YS6B"
access_token <- "54455879-yPWU2L26jrBGSuh5duFe10aE3KkTlCI17lftzBOUw"
access_token_secret <- "TCG72FRR5x3NUEtiFdv5Nq5fbUasenbiUJSx61Txdk5ub"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
datascience_tweets = searchTwitter("datascience", n=10000, lang="en")
library(httr)
install.packages("httr")
install.packages("httr")
library(httr)
oauth_endpoints("twitter")
api_key <- "q0wvm80ceeCGml0UmK7Dl7hvP"
api_secret <- "LE3RvKLuzUncx4T6AjpHNMlhNHABNEtVg9lriiJWLHyD18YS6B"
access_token <- "54455879-yPWU2L26jrBGSuh5duFe10aE3KkTlCI17lftzBOUw"
access_token_secret <- "TCG72FRR5x3NUEtiFdv5Nq5fbUasenbiUJSx61Txdk5ub"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
datascience_tweets = searchTwitter("datascience", n=10000, lang="en")
install.packages("devtools")
install_github("twitteR", username="geoffjentry")
install_github("twitteR", username="geoffjentry")
install_github("twitteR", username="geoffjentry")
library(devtools)
install_github("twitteR", username="geoffjentry")
library(twitteR)
api_key <- "q0wvm80ceeCGml0UmK7Dl7hvP"
api_secret <- "LE3RvKLuzUncx4T6AjpHNMlhNHABNEtVg9lriiJWLHyD18YS6B"
access_token <- "54455879-yPWU2L26jrBGSuh5duFe10aE3KkTlCI17lftzBOUw"
access_token_secret <- "TCG72FRR5x3NUEtiFdv5Nq5fbUasenbiUJSx61Txdk5ub"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
datascience_tweets = searchTwitter("datascience", n=10000, lang="en")
datacience_txt = sapply(datascience_tweets, function(x) x$getText())
datacience_txt = gsub(“(RT|via)((?:\\b\\W*@\\w+)+)”, “”, datacience_txt)
datacience_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", datacience_txt)
datascience_txt = gsub("@\\w+", "", datascience_txt)
datascience_txt = sapply(datascience_tweets, function(x) x$getText())
datascience_txt = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", datascience_txt)
datascience_txt = gsub("@\\w+", "", datascience_txt)
datascience_txt = gsub("[[:punct:]]", "", datascience_txt)
datascience_txt = gsub("[[:digit:]]", "", bjp_txt)
datascience_txt = gsub("[[:digit:]]", "", datascience_txt)
datascience_txt = gsub("http\\w+", "", datascience_txt)
datascience_txt = gsub("[ \t]{2,}", "", datascience_txt)
datascience_txt = gsub("^\\s+|\\s+$", "", datascience_txt)
datascience_txt = datascience_txt[!is.na(datascience_txt)]
names(datascience_txt) = NULL
names(datascience_txt) = NULL
require(devtools)
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
require(sentiment)
ls("package:sentiment")
require(sentiment)
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
install.packages("Rstem", repos = "http://www.omegahat.org/R")
install.packages("Rstem", repos = "http://www.omegahat.org/R/src/contrib/")
?classify_emotion
install.packages("devtools")
devtools::install_github("mjockers/syuzhet")
install.packages("devtools")
devtools::install_github("mjockers/syuzhet")
install.packages("RCurl")
install.packages("devtools")
devtools::install_github("mjockers/syuzhet")
library(devtools)
install_github("Rstem", username="omegahat")
library(Rstem)
require(devtools)
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
require(sentiment)
library(twitteR)
setwd("~/Documents/WIP/DataSci/Working R/Twitter Sentiment Analysis DataSci")
library(twitteR)
install.packages("twitteR")
library(twitteR)
install.packages("twitteR")
install.packages("ggplot2", lib="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
datacience.tweets = searchTwitter('#datascience', n=10000)
install.packages("wordcloud")
install.packages("tm")
library("wordcloud")
library("tm")
consumer_key <- 'q0wvm80ceeCGml0UmK7Dl7hvP'
consumer_secret <- 'LE3RvKLuzUncx4T6AjpHNMlhNHABNEtVg9lriiJWLHyD18YS6B'
access_token <- '54455879-yPWU2L26jrBGSuh5duFe10aE3KkTlCI17lftzBOUw'
access_secret <- 'TCG72FRR5x3NUEtiFdv5Nq5fbUasenbiUJSx61Txdk5ub'
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
1
datacience_tweets <- searchTwitter("#datascience", n=10000)
length(datacience_tweets)
datacience_tweets_text <- sapply(datacience_tweets, function(x) x$getText())
datacience_tweets_text_corpus <- Corpus(VectorSource(datacience_tweets_text))
datacience_tweets_text_corpus <- tm_map(datacience_tweets_text_corpus, content_transformer(tolower))
datacience_tweets_text_corpus <- tm_map(datacience_tweets_text_corpus,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
mc.cores=1
)
datacience_tweets_text_corpus <- tm_map(datacience_tweets_text_corpus,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
mc.cores=1
)
datacience_tweets <- searchTwitter("#datascience", n=10000)
